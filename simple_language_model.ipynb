{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be64c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3743b",
   "metadata": {},
   "source": [
    "Map all sentences characters to an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd857055",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "chars = set(''.join(text))\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd86227e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': 0,\n",
       " 'e': 1,\n",
       " ' ': 2,\n",
       " 'd': 3,\n",
       " 'f': 4,\n",
       " 'u': 5,\n",
       " 'w': 6,\n",
       " 'r': 7,\n",
       " 'm': 8,\n",
       " 'v': 9,\n",
       " 'o': 10,\n",
       " 'n': 11,\n",
       " 'i': 12,\n",
       " 'a': 13,\n",
       " 'g': 14,\n",
       " 'c': 15,\n",
       " 'y': 16}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae83f1",
   "metadata": {},
   "source": [
    "Ensure the string length to be equal to the longest size, this will allow we train the model in batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06fb46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = len(max(text, key=len))\n",
    "\n",
    "for i in range(len(text)):\n",
    "  while len(text[i])<maxlen:\n",
    "      text[i] += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71fe8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8eb089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n"
     ]
    }
   ],
   "source": [
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "  input_seq.append(text[i][:-1])\n",
    "  target_seq.append(text[i][1:])\n",
    "  print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c66e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a0bd23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)    \n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b576c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a454df",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c989e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee9437ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_s, output, hidden, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(input_s, hidden, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        out = out.contiguous().view(-1, self.hidden)\n",
    "        return self.fc(out), hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b14d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_s=dict_size, output=dict_size, hidden=12, n_layers=1)\n",
    "model.to(device)\n",
    "\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80c723b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 2.4897\n",
      "Epoch: 20/100............. Loss: 2.1467\n",
      "Epoch: 30/100............. Loss: 1.8216\n",
      "Epoch: 40/100............. Loss: 1.4637\n",
      "Epoch: 50/100............. Loss: 1.1126\n",
      "Epoch: 60/100............. Loss: 0.7986\n",
      "Epoch: 70/100............. Loss: 0.5484\n",
      "Epoch: 80/100............. Loss: 0.3835\n",
      "Epoch: 90/100............. Loss: 0.2749\n",
      "Epoch: 100/100............. Loss: 0.2048\n"
     ]
    }
   ],
   "source": [
    "def training_():\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        optimizer.zero_grad() \n",
    "        input_seq.to(device)\n",
    "        output, hidden = model(input_seq)\n",
    "        loss = criterion(output, target_seq.view(-1).long())\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        if epoch%10 == 0:\n",
    "            print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "            print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "training_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d621df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, character):\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden\n",
    "\n",
    "def sample_valuation(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57e56671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good i am fine '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_valuation(model, 15, 'Go')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be529b",
   "metadata": {},
   "source": [
    "We use one hot encoding to represent the textual data, because its computationaly expensive and it is not have embedded information  this approach have many downsides.\n",
    "\n",
    "Most modern NLP solutions rely on word embeddings or unique contextual word representation.(eg. word2vec, bert)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
